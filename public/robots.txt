# robots.txt for ZurichJS Conference 2026
# https://conf.zurichjs.com

# Default rules for all crawlers
User-agent: *
Allow: /

# Disallow admin and transactional pages
Disallow: /admin
Disallow: /api/
Disallow: /cart
Disallow: /success
Disallow: /manage-order
Disallow: /validate/

# ============================================
# AI Crawler Rules - Explicitly Allow for Discoverability
# These rules ensure AI systems like ChatGPT, Claude, and others
# can access content to include in AI search results
# ============================================

# OpenAI ChatGPT crawlers
User-agent: GPTBot
Allow: /

User-agent: ChatGPT-User
Allow: /

# Anthropic Claude crawlers
User-agent: anthropic-ai
Allow: /

User-agent: Claude-Web
Allow: /

User-agent: ClaudeBot
Allow: /

# Google AI crawlers
User-agent: Google-Extended
Allow: /

# Perplexity AI
User-agent: PerplexityBot
Allow: /

# Microsoft/Bing AI
User-agent: Bingbot
Allow: /

# Meta AI
User-agent: FacebookBot
Allow: /

User-agent: meta-externalagent
Allow: /

# Apple Intelligence
User-agent: Applebot-Extended
Allow: /

# Common AI research crawlers
User-agent: CCBot
Allow: /

User-agent: cohere-ai
Allow: /

# ============================================
# Important Files
# ============================================

# Sitemap location
Sitemap: https://conf.zurichjs.com/sitemap.xml

# LLM-specific content file for AI discoverability
# See: https://llmstxt.org
# Location: https://conf.zurichjs.com/llms.txt
